# Hermes ‚Äì Assistente Pessoal Offline

Assistente pessoal modular, privado e offline com m√∫ltiplos usu√°rios, interface visual, entrada por voz/texto e integra√ß√£o com LLM local.

## Instala√ß√£o

Instale o Hermes em modo edit√°vel para desenvolver ou executar localmente:

```bash
pip install -e .
```

Caso deseje instalar apenas as depend√™ncias listadas, utilize `requirements.txt`:

```bash
pip install -r requirements.txt
```

O arquivo de requisitos inclui o pacote `scikit-learn` (>=1.1),
necess√°rio para a funcionalidade de busca sem√¢ntica de ideias.

### Depend√™ncias opcionais

Algumas funcionalidades podem requerer bibliotecas adicionais que n√£o
est√£o inclu√≠das na instala√ß√£o padr√£o:

- `vosk` ‚Äì reconhecimento de fala. **Obrigat√≥rio** para utilizar a entrada
  por voz.
- `ollama` ‚Äì cliente Python para o servidor Ollama.

Para usar voz, instale o `vosk` e baixe o modelo de linguagem [pt-BR pequeno](https://alphacephei.com/vosk/models/vosk-model-small-pt-0.3.zip).
Descompacte o conte√∫do em `~/.cache/vosk/vosk-model-small-pt-0.3` para que o
aplicativo consiga localizar os arquivos offline.

Instale as depend√™ncias manualmente caso deseje experimentar esses recursos.

## Servidor LLM

Para funcionalidades que usam o modelo de linguagem √© necess√°rio que um
servidor esteja ativo em `http://localhost:11434`. Uma op√ß√£o √© rodar
o [Ollama](https://github.com/jmorganca/ollama) localmente:

```bash
ollama serve
```

### Configura√ß√£o

Os principais par√¢metros da aplica√ß√£o podem ser ajustados via vari√°veis de
ambiente ou argumentos de linha de comando. Valores padr√£o:

| Par√¢metro       | Vari√°vel de ambiente     | Argumento CLI     | Padr√£o                  |
|-----------------|--------------------------|-------------------|-------------------------|
| Caminho do banco| `HERMES_DB_PATH`         | `--db-path`       | `hermes.db`             |
| Porta do servidor LLM | `HERMES_API_PORT`   | `--api-port`      | `11434`                 |
| Modelo Ollama   | `HERMES_OLLAMA_MODEL`    | `--ollama-model`  | `mistral`               |
| Timeout (s)     | `HERMES_TIMEOUT`         | `--timeout`       | `30`                    |

O endpoint utilizado para comunica√ß√£o com o LLM √© constru√≠do a partir da
porta (`http://localhost:<porta>/api/generate`). Os argumentos de linha de
comando podem ser passados ao executar `python -m hermes` ou `python -m
hermes.ui.cli`.

Esses valores tamb√©m podem ser informados diretamente ao chamar a fun√ß√£o
`gerar_resposta`:

```python
from hermes.services.llm_interface import gerar_resposta
resposta = gerar_resposta("Oi?", url="http://meu-servidor:port/api/generate", model="outro-modelo")
```

## Execu√ß√£o

### CLI
O modo em linha de comando inicia o fluxo principal da aplica√ß√£o:

```bash
python -m hermes.ui.cli
```

### Interface gr√°fica
Para abrir a interface gr√°fica (PyQt5):

```bash
python -m hermes
```

Na interface, os campos de t√≠tulo e descri√ß√£o possuem um bot√£o de microfone
("üéôÔ∏è") que permite ditar texto. Ao salvar uma ideia com sucesso, o Hermes
fornece um breve feedback em voz dizendo "ideia salva".

### Pesquisa sem√¢ntica
A aplica√ß√£o oferece uma pesquisa de ideias baseada em similaridade
sem√¢ntica. Ap√≥s instalar as depend√™ncias, importe e utilize a fun√ß√£o
`semantic_search`:

```python
from hermes.services.semantic_search import semantic_search

resultados = semantic_search("kanban", user_id=1)
for ideia in resultados:
    print(ideia["title"])
```

No modo CLI, selecione a op√ß√£o **Pesquisar ideias** e informe o termo desejado.

O mecanismo utiliza por padr√£o um √≠ndice simples baseado em TF-IDF.  Para
substitu√≠-lo por solu√ß√µes mais avan√ßadas como `FAISS` ou `Chroma`, implemente a
interface ``VectorIndex`` e forne√ßa a inst√¢ncia ao chamar ``semantic_search``:

```python
from hermes.services.semantic_search import semantic_search, VectorIndex

class MeuIndice(VectorIndex):
    ...  # implemente fit() e search()

resultado = semantic_search("kanban", index=MeuIndice())
```

## Testes

Os testes automatizados utilizam o m√≥dulo `unittest` padr√£o do Python.
Execute todos eles com:

```bash
python -m unittest discover -s tests
```

## Desenvolvimento

Instale e configure os linters com [pre-commit](https://pre-commit.com/):

```bash
pip install pre-commit
pre-commit install
```

Consulte tamb√©m [CONTRIBUTING.md](CONTRIBUTING.md) para o fluxo de contribui√ß√£o,
[CHANGELOG.md](CHANGELOG.md) para o hist√≥rico de mudan√ßas e
[LICENSE](LICENSE) para os termos de licen√ßa.

